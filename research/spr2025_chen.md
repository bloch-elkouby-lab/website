---
title: "Spring 2025 Presentation - Chen"
permalink: /presentations/spr2025_chen/
---
## {% include icon.html icon="fa-solid fa-feather-pointed" %}

## How much do inappropriate responses impede clinicians’ engagement and therapeutic alliance with a virtual patient? ##

by **Jimmy Chen**, Jade Wei, Shibani Datta, Runqin Shi, Sarah Bloch-Elkouby

### Background: ###
Leveraging technology to augment and transform the process of psychotherapy has been a constant trend of efforts, from the use of telephone decades ago to the artificial intelligence (AI) today. The concerns have risen at the same time, cautioning that the use technology will lead to miscommunication due to missing non-verbal cues, inhibit inspirational thinking associated with self-discovery, and the quality of therapeutic alliance (Anthony, 2003). While these concerns remain valid and difficult to remedy, determining the impact of technological shortcomings becomes a vital index for the feasibility of such technology.

The virtual patient (VP) can be used as a pedagogic tool for diagnostic psychiatric interviews. As an emerging technology, it remained inferior to trained human actors in perceived authenticity, leading to lower diagnostic accuracy in medical history assessments (Fink et al., 2021). However, the technology offers scalability and consistency, which is difficult for human actors in a psychiatric setting. A study conducted in 2016 with preliminary VP technology recruited 36 participants to conduct an interview on a VP with depression symptoms
(Dupuy et. Al, 2020). Although the VP received positive comments overall, it only offered predetermined options as conversational input
and corresponding pre-recorded outputs. 

The development of artificial intelligence provided an unprecedented opportunity that enables VPs to engage in free conversations. From rule-
based chatbots before 2000s to neural network-based intent-matching around 2010s to transformer based-large language models currently, the
evolution has significantly enhanced the flexibility and sophistication of conversational agents (Al-Amin, 2024).

### Objective: ###

The current study seeks to investigate the feasibility of utilizing a VP in training new and future clinicians. Specifically, despite the sophistication of VP technology, it still produces a substantial number of mistakes that impede the flow of conversation, sabotage therapeutic alliance, and interfere with research implementation, measurements, and educational effectiveness of the simulated interaction.
Aims
1. Quantitatively measure the impact of technical errors in the conversation on therapeutic alliance perceived by the clinician
2. Explore whether such impact is moderated by the race of VP.

Hypotheses
1. The number of technical errors would be positively correlated with the rupture of therapeutic alliance and negatively corrected with perceived alliance.
2. Interviewing with Black Noah will exacerbate the damage of technical errors on therapeutic alliance.

### Methods: ###

The current study is a part of a larger study that utilizes VP to investigate the impact of racial bias on clinical judgement, therapy
process on clinical interviewing and suicide risk assessment. The VP in the current study was developed based on Google Dialogflow, which is a natural language processing platform based on intent matching (Sabharwal & Agrawal, 2020). An extensive list of intents are created to comprehensively simulate Noah according to common aspects of inquiry in a diagnostic interview. Each intent is associated with a pre-scripted response, written by master-level research assistants and edited and approved by a psychologist with an expertise in suicide prevention. To improve realism, Noah respond in both text and voice. The voice and video are AI-generated and pre-recorded. There are two versions of Noah differed only by the video image: Black and White.

### Result: ###

On average, the diagnostic interviews lasted 30.77 minutes (SD = 14.084), generated 15.3 questions without a matching intent (SD=10.431), and 11.37 questions with a nonsensical response (SD=7.729). The working together subscale, which is calculated by averaging its 5 items, centered around a neutral response of “somewhat” (m=4.062, SD=1.145). Similarly, the internal rupture impact score centered around “some negative impact” (m=4.400, SD=1.288). 

Preliminary correlational analysis (Peason’s r) revealed strongnegative correlation between the two outcome variables (see poster below). Yet they remained different enough to be tested as separate construal. There are substantial moderate to strong positive correlations among the predictors. 

Two regression analyses were conducted for aim 1 with 2 outcome (working together: R2=0.117, p=.022; internal rupture: R2=0.086, p=.106). Working together was significantly predicted by disruptive languages (β=-0.209, p=.018) and tech question (β=0.016, p=.021). As the perceived disruption in languages increases by 1 point out of 6 points, the average score for working together decreases by 0.209 point out 7-points. As the number of tech question increase by 1, the average score for working together increases by 0.016 point. The internal rupture was significantly predicted by disruptive languages (β=0.219, p=.040). As the perceived disruption in languages increases by 1 point, the rupture score increases by 0.219 out of 7 point.

Another two regression analyses included the additional moderation terms for aim 2 (working together: R2=0.177, p=.021; internal rupture: R2=0.116, p=.233). Working together was predicted by intent no match (β=0.033, p=.034) and the interaction between intent mismatch and VP race (β=0.078, p=.010). As the number intent no match increases by 1, the average score for working together increases by 0.033 point. Compared to the Black Noah, one point increase in intent mismatch further increases the working together average by 0.078 point when the clinician interviewed the White Noah.

### Poster ###
<img src="bloch-elkouby-lab/website/images/JChen-Poster.jpg" >